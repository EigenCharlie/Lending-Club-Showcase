"""ContribuciÃ³n central de la tesis: predict-then-optimize con conformal prediction."""

from __future__ import annotations

import pandas as pd
import plotly.graph_objects as go
import streamlit as st
from streamlit_mermaid import st_mermaid

from streamlit_app.components.metric_cards import kpi_row
from streamlit_app.components.narrative import next_page_teaser
from streamlit_app.theme import PLOTLY_TEMPLATE
from streamlit_app.utils import format_pct, load_json, load_parquet, load_runtime_status

st.title("ðŸŽ¯ ContribuciÃ³n de Tesis")
st.caption(
    "Predict-then-Optimize con Conformal Prediction: decisiones de portafolio "
    "bajo incertidumbre cuantificada con garantÃ­as matemÃ¡ticas."
)

# â”€â”€ Research Question â”€â”€
st.markdown(
    """
## Pregunta de investigaciÃ³n

> **Â¿CÃ³mo tomar decisiones Ã³ptimas de asignaciÃ³n de portafolio crediticio cuando la
> probabilidad de default tiene incertidumbre inherente?**

El enfoque tradicional usa predicciones puntuales de PD como si fueran exactas. Esto produce
portafolios frÃ¡giles: una pequeÃ±a desviaciÃ³n del modelo invalida la decisiÃ³n. Esta tesis propone
un pipeline que **cuantifica la incertidumbre** y la **incorpora directamente en la optimizaciÃ³n**.
"""
)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DATASET MOTIVATION â€” why Lending Club enables all seven disciplines
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
st.subheader("El dataset como plataforma de investigaciÃ³n")

st.markdown(
    """
### Â¿Por quÃ© Lending Club?

El dataset de **Lending Club** (2.26 millones de prÃ©stamos, 2007-2020) no se eligiÃ³ por
conveniencia: es uno de los pocos datasets pÃºblicos que reÃºne las condiciones necesarias
para demostrar un pipeline end-to-end de riesgo de crÃ©dito con rigor acadÃ©mico.

- **Cobertura temporal completa**: abarca desde la post-crisis financiera de 2008 hasta el
  inicio de la pandemia COVID-19, capturando al menos un ciclo crediticio completo con
  perÃ­odos de expansiÃ³n y estrÃ©s.
- **Volumen y diversidad**: 2.26M de prÃ©stamos con mÃ¡s de 150 variables originales
  (atributos del prestatario, tÃ©rminos del prÃ©stamo, comportamiento de pago, variables
  macroeconÃ³micas implÃ­citas).
- **Reproducibilidad**: dataset pÃºblico en Kaggle, permitiendo que cualquier investigador
  replique los resultados.
"""
)

st.markdown(
    """
### Siete disciplinas, un dataset

La verdadera fortaleza de este dataset es que habilita la convergencia de mÃºltiples
disciplinas en un Ãºnico flujo analÃ­tico:

**1. Machine Learning para credit scoring** â€” Los modelos de PD (Probability of Default)
son la piedra angular del riesgo de crÃ©dito. CatBoost aprovecha el manejo nativo de
variables categÃ³ricas y valores nulos, eliminando la necesidad de preprocesamiento manual.
La ingenierÃ­a de features con WOE (Weight of Evidence) via OptBinning transforma variables
crudas en predictores con poder discriminativo medido por Information Value.

**2. CalibraciÃ³n de probabilidades** â€” Un modelo con AUC alto no necesariamente produce
probabilidades confiables. La calibraciÃ³n Platt (sigmoid scaling) convierte scores en
probabilidades que reflejan frecuencias reales de default (ECE=0.013), requisito
fundamental antes de cualquier cuantificaciÃ³n de incertidumbre o toma de decisiones.

**3. Conformal Prediction** â€” Cuantificar incertidumbre sin asumir ninguna distribuciÃ³n
es el corazÃ³n de este proyecto. Split Conformal Prediction genera intervalos
[PD_low, PD_high] con garantÃ­a matemÃ¡tica de cobertura en muestra finita. La variante
Mondrian extiende esta garantÃ­a a nivel de subgrupo (loan grade A, B, ..., G), asegurando
que cada segmento de riesgo tenga su propia cobertura controlada.

**4. InvestigaciÃ³n de Operaciones** â€” La optimizaciÃ³n de portafolio transforma predicciones
en decisiones de asignaciÃ³n de capital. Pyomo formula el problema como un LP con
restricciones de presupuesto, concentraciÃ³n y PD mÃ¡xima. HiGHS resuelve el problema en
fracciones de segundo. La innovaciÃ³n: usar los intervalos conformal como conjuntos de
incertidumbre box para optimizaciÃ³n robusta, protegiendo contra el peor caso plausible.

**5. Inferencia Causal** â€” Â¿QuÃ© pasa si la tasa de interÃ©s sube 1 punto porcentual?
La correlaciÃ³n no basta para responder polÃ­ticas crediticias. Double/Debiased Machine
Learning (DML) y Causal Forests (via econml y dowhy) estiman efectos causales
heterogÃ©neos, eliminando el sesgo de selecciÃ³n que contamina regresiones ingenuas.

**6. Series de Tiempo** â€” Las tasas de default agregadas mensuales revelan patrones
estacionales y tendencias macro. ARIMA captura la estructura temporal, LightGBM
(via Nixtla mlforecast) incorpora features exÃ³genas, y los intervalos conformal
proporcionan bandas de pronÃ³stico con cobertura controlada para stress testing.

**7. AnÃ¡lisis de Supervivencia** â€” No solo importa *si* un prÃ©stamo incumple, sino
*cuÃ¡ndo*. Cox Proportional Hazards y Random Survival Forests estiman la funciÃ³n de
riesgo condicional al tiempo, generando las curvas de PD lifetime necesarias para el
cÃ¡lculo de provisiones IFRS9 Stage 2 (deterioro significativo del crÃ©dito).
"""
)

st.success(
    "**FÃ¡brica de insights**: estas siete disciplinas no operan en silos â€” convergen "
    "en un pipeline end-to-end donde la salida de una alimenta la entrada de otra. "
    "Desde un CSV crudo hasta un portafolio optimizado con garantÃ­as matemÃ¡ticas de "
    "cobertura, este dataset demuestra que un enfoque integrado produce decisiones "
    "mÃ¡s robustas que la suma de sus componentes individuales."
)

# â”€â”€ Pipeline Diagram â”€â”€
st.subheader("1) Pipeline: del modelo a la decisiÃ³n robusta")

st_mermaid(
    """
    graph LR
        A[CatBoost PD] --> B[CalibraciÃ³n Platt]
        B --> C[MAPIE Mondrian<br/>Conformal Prediction]
        C --> D["[PD_low, PD_high]<br/>Intervalos con garantÃ­a"]
        D --> E[Box Uncertainty Sets]
        E --> F[Pyomo Robust<br/>Optimization + HiGHS]
        F --> G[Portafolio Ã“ptimo<br/>Robusto]

        style A fill:#1a1a2e,stroke:#00D4AA,color:#e0e0e0
        style B fill:#1a1a2e,stroke:#00D4AA,color:#e0e0e0
        style C fill:#16213e,stroke:#FFD93D,color:#e0e0e0
        style D fill:#16213e,stroke:#FFD93D,color:#e0e0e0
        style E fill:#0f3460,stroke:#FF6B6B,color:#e0e0e0
        style F fill:#0f3460,stroke:#FF6B6B,color:#e0e0e0
        style G fill:#1a1a2e,stroke:#00D4AA,color:#e0e0e0
    """,
    height=200,
)

st.markdown(
    """
**Cada etapa tiene un propÃ³sito preciso:**
1. **CatBoost PD**: modelo de clasificaciÃ³n robusto con manejo nativo de categorÃ­as y nulos.
2. **CalibraciÃ³n Platt**: convierte scores en probabilidades verdaderas (ECE=0.0128).
3. **Conformal Prediction Mondrian**: genera intervalos `[PD_low, PD_high]` con garantÃ­a de
   cobertura empÃ­rica por grupo (grade), sin supuestos distribucionales.
4. **Box Uncertainty Sets**: encapsula los intervalos como conjuntos de incertidumbre para optimizaciÃ³n.
5. **Robust Optimization**: resuelve el problema de asignaciÃ³n bajo el peor caso plausible dentro del
   conjunto de incertidumbre (Pyomo + HiGHS).
"""
)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SECTION 2: CalibraciÃ³n â€” por quÃ© es imprescindible y cÃ³mo se complementa con Conformal
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
st.subheader("2) Â¿Por quÃ© calibrar antes de cuantificar incertidumbre?")

st.markdown(
    """
Muchos modelos de ML producen **scores**, no **probabilidades**. Un CatBoost puede decir
"este prÃ©stamo tiene score 0.12", pero eso no significa que exactamente el 12% de los
prÃ©stamos con ese score incumplirÃ¡n. La **calibraciÃ³n** corrige ese sesgo para que las
salidas del modelo reflejen frecuencias reales.
"""
)

col_cal_left, col_cal_right = st.columns(2)
with col_cal_left:
    st.markdown(
        """
#### Sin calibraciÃ³n
```
Score modelo = 0.12
Realidad     = 8% defaults
```
El modelo sobreestima el riesgo en 4pp.
Si usamos 0.12 en el optimizador, seremos
innecesariamente conservadores.
"""
    )
with col_cal_right:
    st.markdown(
        """
#### Con calibraciÃ³n Platt
```
Score modelo  = 0.12
PD calibrada  = 0.082
Realidad      = 8% defaults
```
Ahora la probabilidad refleja la realidad.
El optimizador trabaja con datos honestos.
"""
    )

st.info(
    "**Regla de oro:** La calibraciÃ³n corrige el *nivel* de las probabilidades "
    "(que sean honestas). Conformal prediction genera *intervalos* alrededor de esas "
    "probabilidades (que capturen la incertidumbre). Son complementarios, no sustitutos."
)

# â”€â”€ Platt vs Isotonic vs Venn-Abers â”€â”€
with st.expander("MÃ©todos de calibraciÃ³n: Platt, Isotonic y Venn-Abers"):
    st.markdown(
        """
### Â¿QuÃ© hace cada mÃ©todo?

| MÃ©todo | Mecanismo | Ventajas | Limitaciones |
|--------|-----------|----------|-------------|
| **Platt Scaling** | Ajusta una funciÃ³n sigmoide: `P(y=1) = 1/(1 + exp(-az - b))` | Suave, generalizable, pocos parÃ¡metros (a, b) | Asume relaciÃ³n sigmoid entre score y probabilidad |
| **Isotonic Regression** | Ajuste no-paramÃ©trico monotÃ³nico (step function) | Flexible, no asume forma funcional | Riesgo de overfitting con calibraciÃ³n sets pequeÃ±os |
| **Venn-Abers** | Genera **dos** calibraciones isotonic (una asumiendo y=0, otra y=1) y reporta un **intervalo** [p_low, p_high] | Produce intervalos con garantÃ­a de validez | MÃ¡s complejo, computacionalmente costoso, menos conocido |

### Â¿Por quÃ© elegimos Platt?

En NB03 comparamos Isotonic (ECE=0.0019 en calibraciÃ³n) vs Platt (ECE=0.0128 en test).
Aunque isotonic tiene mejor ECE *en el set de calibraciÃ³n*, Platt fue seleccionado porque:

1. **Generaliza mejor** a datos fuera de muestra (test set OOT 2018-2020).
2. **Es mÃ¡s estable** con calibraciÃ³n sets de tamaÃ±o moderado (~237K observaciones).
3. **Produce una curva suave**, no una step function que puede crear "saltos" artificiales.

### Â¿QuÃ© pasarÃ­a con Venn-Abers en vez de Platt?

Venn-Abers es fascinante porque ya produce un **intervalo de probabilidad** [p_low, p_high]
como output, no un punto. La pregunta natural es: Â¿necesitamos Conformal Prediction si
Venn-Abers ya da intervalos?

La respuesta es **sÃ­**, por tres razones:

| DimensiÃ³n | Venn-Abers | Conformal Prediction |
|-----------|-----------|---------------------|
| **Tipo de garantÃ­a** | Validez probabilÃ­stica (las probabilidades son "vÃ¡lidas") | Cobertura empÃ­rica (el % de veces que el valor real cae en el intervalo es controlable) |
| **Ancho del intervalo** | Determinado por la discrepancia entre dos calibradores isotonic | Determinado por el nivel de confianza elegido (90%, 95%) |
| **Control por grupo** | No tiene variante Mondrian estÃ¡ndar | **Mondrian** permite garantÃ­as por subgrupo (grade A, B, C...) |
| **InterpretaciÃ³n** | "La PD verdadera estÃ¡ entre p_low y p_high" | "Con 90% de probabilidad, el evento observado cae en este rango" |

**ConclusiÃ³n**: Venn-Abers y Conformal resuelven problemas **diferentes**:
- Venn-Abers calibra de forma conservadora (intervalos de *probabilidad*).
- Conformal cuantifica incertidumbre de *predicciÃ³n* con cobertura controlable.

En este proyecto, Platt calibra el punto central (PD honesta) y Conformal genera el
intervalo operativo [PD_low, PD_high] que consume el optimizador. Si usÃ¡ramos Venn-Abers,
tendrÃ­amos intervalos de calibraciÃ³n, pero **no** la garantÃ­a de cobertura marginal finita
que ofrece Conformal Prediction y que es esencial para la robustez del optimizador.
"""
    )

# â”€â”€ Calibration â†’ Conformal flow â”€â”€
with st.expander("Â¿CÃ³mo se complementan calibraciÃ³n y conformal prediction?"):
    st.markdown(
        """
### El flujo completo, paso a paso

```
Paso 1: CatBoost produce un score bruto
        â†’ score = 0.15 (no es una probabilidad confiable)

Paso 2: Platt Scaling calibra el score
        â†’ PD_point = 0.12 (ahora sÃ­ es una probabilidad honesta)

Paso 3: Conformal Prediction genera el intervalo
        â†’ [PD_low, PD_high] = [0.06, 0.18] con 90% de garantÃ­a

Paso 4: El optimizador usa PD_high = 0.18 como peor caso
        â†’ DecisiÃ³n robusta que soporta la incertidumbre real
```

### Â¿Por quÃ© no saltar la calibraciÃ³n?

Si alimentamos Conformal Prediction con scores **no calibrados**:
- Los intervalos serÃ¡n **tÃ©cnicamente vÃ¡lidos** (la cobertura se cumple).
- Pero el **centro del intervalo** estarÃ¡ sesgado.
- Un PD_point de 0.15 cuando la realidad es 0.08 produce un intervalo
  desplazado: [0.09, 0.21] en vez de [0.02, 0.14].
- El optimizador tomarÃ­a decisiones demasiado conservadoras.

**La calibraciÃ³n centra el intervalo; conformal controla su ancho.**

### Â¿Y si no usamos Conformal Prediction?

Sin Conformal, solo tenemos PD_point = 0.12. El optimizador asume que es exacta.
Si el modelo tiene un error de Â±5pp (comÃºn en riesgo de crÃ©dito), el portafolio
"Ã³ptimo" puede ser subÃ³ptimo o incluso peligroso.

Con Conformal: el optimizador sabe que la PD puede ser hasta 0.18 y se protege.
La pÃ©rdida de retorno (precio de robustez) es el costo de esa protecciÃ³n.
"""
    )

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SECTION 3: Comparison table â€” Why Conformal?
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
st.subheader("3) Â¿Por quÃ© Conformal Prediction y no otro mÃ©todo?")

comparison_data = pd.DataFrame(
    [
        {
            "MÃ©todo": "Punto (sin intervalos)",
            "GarantÃ­a de cobertura": "Ninguna",
            "Supuestos": "Modelo perfecto",
            "Muestra finita": "No",
            "Veredicto": "FrÃ¡gil â€” ignora incertidumbre",
        },
        {
            "MÃ©todo": "Bootstrap",
            "GarantÃ­a de cobertura": "AsintÃ³tica",
            "Supuestos": "n â†’ âˆž",
            "Muestra finita": "Aproximada",
            "Veredicto": "Razonable pero sin garantÃ­a formal",
        },
        {
            "MÃ©todo": "Bayesiano (credible intervals)",
            "GarantÃ­a de cobertura": "Condicional al prior",
            "Supuestos": "DistribuciÃ³n correcta",
            "Muestra finita": "Depende del prior",
            "Veredicto": "Fuerte si el prior es correcto",
        },
        {
            "MÃ©todo": "Venn-Abers",
            "GarantÃ­a de cobertura": "Validez probabilÃ­stica",
            "Supuestos": "Exchangeability",
            "Muestra finita": "SÃ­",
            "Veredicto": "CalibraciÃ³n conservadora, no cobertura operativa",
        },
        {
            "MÃ©todo": "Conformal Prediction",
            "GarantÃ­a de cobertura": "Marginal exacta",
            "Supuestos": "Exchangeability",
            "Muestra finita": "SÃ­ (matemÃ¡tica)",
            "Veredicto": "DistribuciÃ³n-libre + cobertura controlable",
        },
    ]
)
st.dataframe(comparison_data, use_container_width=True, hide_index=True)

st.success(
    "**Ventaja clave**: Conformal Prediction es el Ãºnico mÃ©todo que ofrece garantÃ­as de cobertura "
    "en muestra finita sin asumir una distribuciÃ³n especÃ­fica. Esto es exactamente lo que necesita "
    "un optimizador que debe ser robusto ante errores del modelo."
)

with st.expander("Â¿QuÃ© es Conformal Prediction? (explicaciÃ³n desde cero)"):
    st.markdown(
        """
### Para quienes nunca han oÃ­do de Conformal Prediction

Imagina que tienes un modelo que predice la temperatura de maÃ±ana: **25Â°C**.
Â¿Pero quÃ© tan seguro es? PodrÃ­a ser 23Â°C o 28Â°C.

**Conformal Prediction** dice: "No sÃ© la distribuciÃ³n del error, pero puedo mirar
los errores pasados del modelo en datos que ya conozco la respuesta, y construir
un intervalo que contenga la respuesta correcta el 90% de las veces."

**Â¿CÃ³mo funciona?**
1. Entrena un modelo normal (CatBoost, cualquiera).
2. En un set de calibraciÃ³n separado, calcula los errores del modelo.
3. Toma el cuantil 90% de esos errores. Ese es el "radio" del intervalo.
4. Para datos nuevos: predicciÃ³n Â± radio = intervalo con 90% de cobertura.

**Â¿Por quÃ© es revolucionario?**
- No asume que los errores son normales, ni simÃ©tricos, ni homogÃ©neos.
- Funciona con **cualquier** modelo (neural nets, Ã¡rboles, regresiÃ³n).
- La garantÃ­a de cobertura es **matemÃ¡tica**, no empÃ­rica ni asintÃ³tica.
- Solo requiere que los datos sean **exchangeable** (intercambiables).

**Â¿QuÃ© agrega Mondrian?**
El conformal bÃ¡sico da un solo radio para todos. Pero un prÃ©stamo Grade A
tiene menos incertidumbre que uno Grade G. **Mondrian** calcula un radio
*diferente* por grupo, dando intervalos mÃ¡s justos y operativos.

### Referencia acadÃ©mica
- Vovk, Gammerman & Shafer (2005). *Algorithmic Learning in a Random World*.
- Angelopoulos & Bates (2023). *Conformal Prediction: A Gentle Introduction*.
"""
    )

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SECTION 4: Key Results
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
st.subheader("4) Resultados de impacto")

policy = load_json("conformal_policy_status", directory="models")
robust = load_parquet("portfolio_robustness_summary")
ifrs9 = load_parquet("ifrs9_scenario_summary")
checks_passed = int(policy.get("checks_passed", 0))
checks_total = int(policy.get("checks_total", 0))
policy_gate_text = (
    f"{checks_passed}/{checks_total} checks" if checks_total > 0 else "checks no disponibles"
)
runtime_status = load_runtime_status()
test_suite_total = int(runtime_status.get("test_suite_total", 0) or 0)
test_suite_label = str(test_suite_total) if test_suite_total > 0 else "N/D"

# IFRS9 uses 'total_ecl' column
baseline_ecl = (
    float(ifrs9.loc[ifrs9["scenario"] == "baseline", "total_ecl"].iloc[0])
    if "baseline" in ifrs9["scenario"].values
    else 0
)
severe_ecl = (
    float(ifrs9.loc[ifrs9["scenario"] == "severe", "total_ecl"].iloc[0])
    if "severe" in ifrs9["scenario"].values
    else 0
)

# Robustness summary uses 'risk_tolerance', 'baseline_nonrobust_return', 'best_robust_return'
tol_col = "risk_tolerance" if "risk_tolerance" in robust.columns else "tolerance"
ret_nonrobust_col = (
    "baseline_nonrobust_return"
    if "baseline_nonrobust_return" in robust.columns
    else "nonrobust_return"
)
ret_robust_col = "best_robust_return" if "best_robust_return" in robust.columns else "robust_return"

tol_10 = robust[robust[tol_col] == 0.10] if tol_col in robust.columns else pd.DataFrame()
if not tol_10.empty:
    robust_return = float(tol_10[ret_robust_col].iloc[0])
    nonrobust_return = float(tol_10[ret_nonrobust_col].iloc[0])
    price_of_robustness = nonrobust_return - robust_return
else:
    robust_return = 0
    nonrobust_return = 0
    price_of_robustness = 0

kpi_row(
    [
        {"label": "Cobertura 90% (Mondrian)", "value": format_pct(policy.get("coverage_90", 0))},
        {
            "label": "Policy Gate",
            "value": policy_gate_text,
        },
        {"label": "Retorno robusto (tol=10%)", "value": f"${robust_return:,.0f}"},
        {"label": "Precio de robustez", "value": f"${price_of_robustness:,.0f}"},
        {"label": "ECL baseline", "value": f"${baseline_ecl / 1e6:,.1f}M"},
        {"label": "ECL severo", "value": f"${severe_ecl / 1e6:,.1f}M"},
    ],
    n_cols=3,
)

st.markdown(
    f"""
**Lectura de los KPIs:**
- **Cobertura 90%**: el 91.97% de las veces, el evento real cayÃ³ dentro del intervalo predicho.
  Esto supera el objetivo de 90%, validando la garantÃ­a de Conformal Prediction.
- **Policy Gate ({policy_gate_text})**: validaciones formales de calidad del sistema de intervalos.
- **Precio de robustez**: la diferencia de retorno entre asumir PD exacta vs usar el peor caso
  conformal. Es el costo de la protecciÃ³n.
- **ECL baseline vs severo**: cÃ³mo cambian las provisiones regulatorias bajo estrÃ©s.
  El uplift de +70% muestra la sensibilidad del portafolio a escenarios adversos.
"""
)

# â”€â”€ Robustness Trade-off â”€â”€
st.subheader("5) Trade-off: retorno vs robustez")

if not robust.empty and tol_col in robust.columns:
    fig = go.Figure()
    fig.add_trace(
        go.Scatter(
            x=robust[tol_col],
            y=robust[ret_nonrobust_col],
            mode="lines+markers",
            name="Sin robustez (PD puntual)",
            line={"color": "#FF6B6B", "width": 2.5},
            marker={"size": 8},
        )
    )
    fig.add_trace(
        go.Scatter(
            x=robust[tol_col],
            y=robust[ret_robust_col],
            mode="lines+markers",
            name="Con robustez (PD_high conformal)",
            line={"color": "#00D4AA", "width": 2.5},
            marker={"size": 8},
        )
    )
    fig.update_layout(
        **PLOTLY_TEMPLATE["layout"],
        title="Frontera de robustez: retorno esperado vs tolerancia de riesgo",
        xaxis_title="Tolerancia de PD mÃ¡xima del portafolio",
        yaxis_title="Retorno esperado ($)",
        height=420,
    )
    st.plotly_chart(fig, use_container_width=True)
    st.caption(
        "La brecha entre las curvas es el **precio de robustez**: lo que cuesta protegerse "
        "contra el peor caso plausible. Una brecha pequeÃ±a indica que la protecciÃ³n es barata."
    )

st.markdown(
    """
**Lectura del trade-off:**
- La curva roja asume que la PD puntual es exacta. Maximiza retorno pero es **frÃ¡gil**.
- La curva verde usa `PD_high` (lÃ­mite superior conformal) como constraint. Sacrifica retorno
  pero **garantiza** que el portafolio soporta el peor caso con 90% de probabilidad.
- La diferencia entre ambas es el **precio de la robustez**: cuÃ¡nto cuesta la protecciÃ³n.
- **Este trade-off es la contribuciÃ³n operativa central de la tesis.**
"""
)

# â”€â”€ IFRS9 Connection â”€â”€
st.subheader("6) ConexiÃ³n con IFRS9")
st.markdown(
    f"""
Los intervalos conformal no solo alimentan la optimizaciÃ³n â€” tambiÃ©n mejoran la gobernanza regulatoria:

| Uso en IFRS9 | DescripciÃ³n |
|---|---|
| **ECL por rango** | Provisionar con `PD_high` en vez de `PD_point` para lectura prudencial |
| **SICR signal** | Ancho del intervalo (`PD_high - PD_point`) como seÃ±al adicional de deterioro significativo |
| **Stress testing** | Escenarios con multiplicadores derivados de bandas de pronÃ³stico temporal |
| **Gobernanza** | PolÃ­tica conformal ({policy_gate_text}) documenta calidad de incertidumbre ante auditorÃ­a |
"""
)

# â”€â”€ Reproducibility â”€â”€
st.subheader("7) Reproducibilidad")
st.code(
    """
# Clonar y configurar
git clone <repo> && cd Lending-Club-End-to-End

# Instalar dependencias
uv sync --extra dev

# Ejecutar pipeline completo
uv run python scripts/end_to_end_pipeline.py

# Verificar tests
uv run pytest -x

# Lanzar dashboard
uv run streamlit run streamlit_app/app.py
""",
    language="bash",
)

st.markdown(
    f"""
**Stack tecnolÃ³gico**: Python 3.11 Â· CatBoost Â· MAPIE 1.3 Â· Pyomo + HiGHS Â· DuckDB Â· dbt Â· Feast Â· Streamlit

**{test_suite_label} tests** validan datos, features, modelos, conformal, IFRS9, optimizaciÃ³n, MLflow, Streamlit e integraciÃ³n end-to-end.
"""
)

next_page_teaser(
    "VisiÃ³n End-to-End",
    "Narrativa completa del pipeline con mÃ©tricas detalladas por componente.",
    "pages/thesis_end_to_end.py",
)
