"""Glosario y fundamentos: t√©rminos, t√©cnicas y f√≥rmulas clave del proyecto."""

from __future__ import annotations

import pandas as pd
import streamlit as st

from streamlit_app.components.narrative import next_page_teaser, storytelling_intro
from streamlit_app.utils import format_number, format_pct, load_json, try_load_parquet

st.title("üìñ Glosario y Fundamentos")
st.caption(
    "Referencia r√°pida de todos los conceptos, m√©tricas, t√©cnicas y f√≥rmulas "
    "utilizados en este proyecto de riesgo de cr√©dito end-to-end."
)
st.markdown(
    """
Esta p√°gina funciona como diccionario de consulta. Antes de explorar los resultados anal√≠ticos,
aqu√≠ puedes familiarizarte con los t√©rminos financieros, regulatorios, de machine learning y
optimizaci√≥n que aparecen en todo el recorrido. Cada t√©rmino incluye una definici√≥n accesible
y su conexi√≥n con el proyecto.
"""
)
storytelling_intro(
    page_goal=(
        "Traducir conceptos t√©cnicos de riesgo en lenguaje operativo para lectores no especializados."
    ),
    business_value=(
        "Reduce malentendidos en comit√© y mejora la calidad de decisiones al compartir un vocabulario com√∫n."
    ),
    key_decision=(
        "Definir qu√© m√©tricas y t√©cnicas convienen seg√∫n objetivo: precisi√≥n, prudencia regulatoria o retorno."
    ),
    how_to_read=[
        "Filtra por categor√≠a para no mezclar conceptos de naturaleza distinta.",
        "Usa la columna 'En este proyecto' para conectar teor√≠a con resultados reales.",
        "Consulta la gu√≠a pr√°ctica al final para elegir estrategia seg√∫n contexto.",
    ],
)

# ‚îÄ‚îÄ Glossary Data ‚îÄ‚îÄ
comparison = load_json("model_comparison")
final_metrics = comparison.get("final_test_metrics", {})
best_calibration = str(comparison.get("best_calibration", "N/D"))
hpo_trials = int(comparison.get("hpo_trials_executed", comparison.get("optuna_n_trials", 0)))
policy = load_json("conformal_policy_status", directory="models")
pipeline_summary = load_json("pipeline_summary")
pipeline_metrics = pipeline_summary.get("pipeline", {})
survival_metrics = pipeline_summary.get("survival", {})
ifrs9_baseline = float(pipeline_metrics.get("ecl_expected", 0.0))
ifrs9_severe = float(pipeline_metrics.get("ecl_conservative", 0.0))
ifrs9_uplift = (ifrs9_severe / ifrs9_baseline - 1.0) if ifrs9_baseline else 0.0
nonrobust_return = float(pipeline_metrics.get("nonrobust_return", 0.0))
price_of_robustness = float(pipeline_metrics.get("price_of_robustness", 0.0))
por_pct = (price_of_robustness / (abs(nonrobust_return) + 1e-6) * 100.0) if nonrobust_return else 0.0

GLOSSARY = [
    {
        "termino": "Can√≥nico",
        "categoria": "Gobernanza",
        "definicion": (
            "Fuente oficial (single source of truth) para m√©tricas y decisiones. "
            "Cuando hay varias versiones de un artefacto, la can√≥nica es la que gobierna "
            "reporting, monitoreo y validaci√≥n."
        ),
        "en_proyecto": (
            "Conformal can√≥nico: `models/conformal_results_mondrian.pkl` + "
            "`data/processed/conformal_intervals_mondrian.parquet`."
        ),
    },
    # Financial terms
    {"termino": "PD", "categoria": "Financiero", "definicion": "Probability of Default. Probabilidad de que un pr√©stamo entre en incumplimiento. Es la salida principal del modelo CatBoost calibrado.", "en_proyecto": f"Modelo PD con AUC={final_metrics.get('auc_roc', 0):.4f}, calibrado con {best_calibration} (ECE={final_metrics.get('ece', 0):.4f})."},
    {"termino": "LGD", "categoria": "Financiero", "definicion": "Loss Given Default. Porcentaje del monto expuesto que se pierde cuando ocurre un default. Complemento de la tasa de recuperaci√≥n.", "en_proyecto": "Modelado sobre pr√©stamos en default (~88% nulls esperados en no-defaults)."},
    {"termino": "EAD", "categoria": "Financiero", "definicion": "Exposure at Default. Monto expuesto al momento del incumplimiento. Para pr√©stamos amortizables, es el saldo pendiente.", "en_proyecto": "Dataset especializado ead_dataset.parquet solo con defaults."},
    {"termino": "ECL", "categoria": "Financiero", "definicion": "Expected Credit Loss. P√©rdida esperada = PD √ó LGD √ó EAD √ó Factor de descuento. M√©trica central de IFRS9.", "en_proyecto": f"ECL baseline ${ifrs9_baseline / 1e6:,.1f}M, escenario severo ${ifrs9_severe / 1e9:,.3f}B ({ifrs9_uplift:+.2%})."},
    {"termino": "DTI", "categoria": "Financiero", "definicion": "Debt-to-Income ratio. Pagos mensuales de deuda divididos entre ingreso mensual. Mediana en el dataset: ~15.", "en_proyecto": "Feature clave del modelo PD. DTI alto (>30) se√±ala sobreendeudamiento."},
    {"termino": "NPL", "categoria": "Financiero", "definicion": "Non-Performing Loan. Pr√©stamo con pagos vencidos >90 d√≠as o en reestructuraci√≥n. Equivale a Stage 3 en IFRS9.", "en_proyecto": "Pr√©stamos Charged Off + Default + Late 31-120 en el dataset."},
    {"termino": "SICR", "categoria": "Financiero", "definicion": "Significant Increase in Credit Risk. Evento que dispara la migraci√≥n de Stage 1 a Stage 2 en IFRS9.", "en_proyecto": "Innovaci√≥n: ancho del intervalo conformal (PD_high - PD_point) como se√±al adicional de SICR."},
    {"termino": "Write-off", "categoria": "Financiero", "definicion": "Castigo contable: reconocimiento formal de que un pr√©stamo es irrecuperable. Genera p√©rdida directa en resultados.", "en_proyecto": "Estado 'Charged Off' en el dataset de Lending Club."},
    {"termino": "Spread", "categoria": "Financiero", "definicion": "Diferencia entre la tasa cobrada al prestatario y el costo de fondeo. Representa el margen bruto del pr√©stamo.", "en_proyecto": "int_rate del dataset menos costo de fondeo estimado."},
    {"termino": "Grade", "categoria": "Financiero", "definicion": "Calificaci√≥n de riesgo de Lending Club (A-G). Grade A default ~2%, Grade G ~37%. Variable de mayor poder predictivo.", "en_proyecto": "Usada en Mondrian conformal para intervalos por grupo y en segmentaci√≥n IFRS9."},
    # Regulatory terms
    {"termino": "IFRS 9", "categoria": "Regulatorio", "definicion": "International Financial Reporting Standard 9. Norma contable que requiere provisionar p√©rdidas esperadas (no solo incurridas). Vigente desde enero 2018.", "en_proyecto": "P√°gina completa de provisiones IFRS9 con 4 escenarios y an√°lisis de sensibilidad."},
    {"termino": "Basilea III", "categoria": "Regulatorio", "definicion": "Marco regulatorio bancario que define requerimientos m√≠nimos de capital. Los bancos deben mantener capital suficiente para absorber p√©rdidas inesperadas.", "en_proyecto": "IFRS9 determina provisiones (p√©rdida esperada); Basilea III determina capital (p√©rdida inesperada)."},
    {"termino": "Stage 1", "categoria": "Regulatorio", "definicion": "Pr√©stamos sin deterioro significativo. Se provisiona ECL a 12 meses (PD 12m √ó LGD √ó EAD).", "en_proyecto": "Mayor√≠a del portafolio. PD a 12 meses del modelo CatBoost."},
    {"termino": "Stage 2", "categoria": "Regulatorio", "definicion": "Pr√©stamos con incremento significativo de riesgo (SICR). Se provisiona ECL lifetime (PD lifetime √ó LGD √ó EAD).", "en_proyecto": "Migraci√≥n Stage 1‚Üí2 analizada con conformal width como se√±al SICR."},
    {"termino": "Stage 3", "categoria": "Regulatorio", "definicion": "Pr√©stamos deteriorados (default, 90+ DPD). PD ~ 1.0, se provisiona p√©rdida total esperada.", "en_proyecto": "Pr√©stamos en Charged Off/Default del dataset."},
    {"termino": "Stress Test", "categoria": "Regulatorio", "definicion": "Ejercicio regulatorio que eval√∫a resiliencia de un portafolio bajo escenarios macroecon√≥micos adversos.", "en_proyecto": f"4 escenarios IFRS9 (baseline, mild, adverse, severe) con uplift severo actual {ifrs9_uplift:+.2%}."},
    # ML terms
    {"termino": "AUC", "categoria": "Machine Learning", "definicion": "Area Under the ROC Curve. Mide la capacidad del modelo para separar defaults de no-defaults. 0.5=aleatorio, 1.0=perfecto. En banca, AUC >0.70 se considera aceptable.", "en_proyecto": f"CatBoost calibrado: AUC={final_metrics.get('auc_roc', 0):.4f} en test out-of-time."},
    {"termino": "Gini", "categoria": "Machine Learning", "definicion": "Coeficiente Gini = 2√óAUC - 1. Escala de 0 (sin poder) a 1 (perfecto). M√©trica est√°ndar en credit scoring bancario.", "en_proyecto": f"Gini={final_metrics.get('gini', 0):.4f}, consistente con modelos de cr√©dito al consumo."},
    {"termino": "KS", "categoria": "Machine Learning", "definicion": "Kolmogorov-Smirnov statistic. M√°xima separaci√≥n entre distribuciones acumuladas de buenos y malos. KS >0.30 es buen poder discriminante.", "en_proyecto": f"KS={final_metrics.get('ks_statistic', 0):.4f} en test OOT."},
    {"termino": "Brier Score", "categoria": "Machine Learning", "definicion": "Error cuadr√°tico medio de las probabilidades predichas vs outcomes reales. Menor es mejor. Combina discriminaci√≥n y calibraci√≥n.", "en_proyecto": f"Brier={final_metrics.get('brier_score', 0):.4f} post-calibraci√≥n."},
    {"termino": "ECE", "categoria": "Machine Learning", "definicion": "Expected Calibration Error. Mide qu√© tan bien las probabilidades predichas reflejan las frecuencias reales de default. ECE=0 es calibraci√≥n perfecta.", "en_proyecto": f"ECE={final_metrics.get('ece', 0):.4f} con {best_calibration} (m√©todo seleccionado en validaci√≥n temporal)."},
    {"termino": "SHAP", "categoria": "Machine Learning", "definicion": "SHapley Additive exPlanations. M√©todo de teor√≠a de juegos que atribuye la contribuci√≥n de cada variable a cada predicci√≥n individual.", "en_proyecto": "Top drivers: int_rate, grade, term, loan_to_income, revol_util."},
    {"termino": "CatBoost", "categoria": "Machine Learning", "definicion": "Algoritmo de gradient boosting que maneja variables categ√≥ricas nativamente y es robusto a overfitting. Desarrollado por Yandex. Dominante en competencias de datos tabulares.", "en_proyecto": f"Modelo final: CatBoost tuneado con Optuna ({hpo_trials} trials) + calibraci√≥n {best_calibration}."},
    {"termino": "Gradient Boosting", "categoria": "Machine Learning", "definicion": "T√©cnica de ensamble que construye secuencialmente √°rboles de decisi√≥n, donde cada nuevo √°rbol corrige los errores del anterior.", "en_proyecto": "CatBoost, XGBoost y LightGBM son variantes de gradient boosting."},
    {"termino": "Calibraci√≥n", "categoria": "Machine Learning", "definicion": "Ajuste post-entrenamiento para que las probabilidades predichas sean consistentes con las frecuencias observadas. Si predice PD=10%, ~10% deben hacer default.", "en_proyecto": f"{best_calibration} seleccionada (ECE={final_metrics.get('ece', 0):.4f}) por validaci√≥n temporal multi-m√©trica."},
    {"termino": "Cross-validation", "categoria": "Machine Learning", "definicion": "T√©cnica de evaluaci√≥n que divide datos en K subconjuntos para entrenar y validar el modelo K veces, reduciendo sesgo de evaluaci√≥n.", "en_proyecto": "No usada para split final (se usa OOT temporal), s√≠ para Optuna."},
    {"termino": "WOE", "categoria": "Machine Learning", "definicion": "Weight of Evidence. Transformaci√≥n de variables categ√≥ricas/binneadas que captura la relaci√≥n monot√≥nica con el default. Est√°ndar en credit scoring.", "en_proyecto": "Aplicado a grade, purpose, home_ownership via OptBinning."},
    {"termino": "IV", "categoria": "Machine Learning", "definicion": "Information Value. Mide el poder predictivo global de una variable. IV <0.02 d√©bil, 0.02-0.1 √∫til, 0.1-0.3 fuerte, >0.3 muy fuerte.", "en_proyecto": "Usado para ranking y selecci√≥n de features en NB02."},
    # Uncertainty terms
    {"termino": "Conformal Prediction", "categoria": "Incertidumbre", "definicion": "Marco estad√≠stico que produce intervalos de predicci√≥n con garant√≠a de cobertura finita sin asumir distribuci√≥n param√©trica. Solo requiere intercambiabilidad de datos.", "en_proyecto": f"MAPIE 1.3.0 SplitConformalRegressor. Cobertura 90%={policy.get('coverage_90', 0):.4f}, 95%={policy.get('coverage_95', 0):.4f}."},
    {"termino": "Coverage (Cobertura)", "categoria": "Incertidumbre", "definicion": "Proporci√≥n de valores reales que caen dentro del intervalo predicho. Cobertura 90% = 90% de los valores reales est√°n en [PD_low, PD_high].", "en_proyecto": f"90%: {policy.get('coverage_90', 0):.4f}, 95%: {policy.get('coverage_95', 0):.4f}, checks {int(policy.get('checks_passed', 0))}/{int(policy.get('checks_total', 0))}."},
    {"termino": "Mondrian CP", "categoria": "Incertidumbre", "definicion": "Variante de conformal prediction que calcula intervalos por grupo (e.g., por grade), garantizando cobertura condicional por segmento.", "en_proyecto": f"Intervalos por grade. Min cobertura grupo: {policy.get('min_group_coverage_90', 0):.4f} (meta ‚â•0.88)."},
    {"termino": "Interval Width", "categoria": "Incertidumbre", "definicion": "Ancho del intervalo conformal (PD_high - PD_low). Intervalos m√°s estrechos son m√°s informativos pero con mismo nivel de cobertura.", "en_proyecto": f"Ancho promedio 90%: {policy.get('avg_width_90', 0):.4f} (meta <0.80). Usado como se√±al SICR."},
    {"termino": "Split Conformal", "categoria": "Incertidumbre", "definicion": "M√©todo que usa un conjunto de calibraci√≥n separado para calcular residuos conformales. M√°s eficiente computacionalmente que full conformal.", "en_proyecto": "SplitConformalRegressor con calibration set temporal separado."},
    # Causal terms
    {"termino": "ATE", "categoria": "Causal", "definicion": "Average Treatment Effect. Efecto promedio de una intervenci√≥n sobre toda la poblaci√≥n. Responde: ¬øcu√°nto cambia Y si aplicamos tratamiento T?", "en_proyecto": "+1pp en tasa de inter√©s ‚Üí +0.787pp en probabilidad de default."},
    {"termino": "CATE", "categoria": "Causal", "definicion": "Conditional Average Treatment Effect. Efecto causal que var√≠a por subgrupo/individuo. Permite intervenciones personalizadas.", "en_proyecto": "Distribuci√≥n amplia de CATE justifica pol√≠tica diferenciada por segmento."},
    {"termino": "DML", "categoria": "Causal", "definicion": "Double/Debiased Machine Learning. M√©todo de Chernozhukov et al. (2018) que usa ML para controlar confounders y estimar efectos causales sin sesgo.", "en_proyecto": "EconML LinearDML para estimaci√≥n robusta del efecto tasa ‚Üí default."},
    {"termino": "Causal Forest", "categoria": "Causal", "definicion": "Extensi√≥n de Random Forest para estimar efectos de tratamiento heterog√©neos (CATE). Basado en Athey & Wager (2019).", "en_proyecto": "Modelo de 337MB entrenado para CATE heterog√©neo por segmento."},
    {"termino": "Counterfactual", "categoria": "Causal", "definicion": "Escenario hipot√©tico: ¬øqu√© hubiera pasado si hubi√©ramos aplicado una intervenci√≥n diferente? Base del an√°lisis causal.", "en_proyecto": "Simulaci√≥n contrafactual de pol√≠ticas de intervenci√≥n por regla."},
    # OR terms
    {"termino": "Optimizaci√≥n Robusta", "categoria": "Operations Research", "definicion": "Enfoque de optimizaci√≥n que incorpora incertidumbre en los par√°metros del modelo. En vez de optimizar para el caso esperado, protege contra el peor caso plausible.", "en_proyecto": "PD_high (conformal) como peor caso -> Pyomo/HiGHS resuelve asignaci√≥n robusta."},
    {"termino": "Uncertainty Set", "categoria": "Operations Research", "definicion": "Conjunto de valores posibles para par√°metros inciertos. En optimizaci√≥n robusta, el modelo se protege contra todos los escenarios dentro del conjunto.", "en_proyecto": "[PD_low, PD_high] del conformal define el uncertainty set por pr√©stamo."},
    {"termino": "Price of Robustness", "categoria": "Operations Research", "definicion": "Diferencia en retorno esperado entre la soluci√≥n √≥ptima sin incertidumbre y la robusta. Cuantifica el costo de proteger el downside.", "en_proyecto": f"{por_pct:.2f}% de reducci√≥n de retorno @ tolerancia 0.10 en snapshot actual. Es el 'costo del seguro'."},
    {"termino": "Efficient Frontier", "categoria": "Operations Research", "definicion": "Curva que muestra las mejores combinaciones posibles de riesgo y retorno. No se puede mejorar retorno sin asumir m√°s riesgo.", "en_proyecto": "Frontera eficiente robusta vs no-robusta comparada en la p√°gina de portafolio."},
]

# ‚îÄ‚îÄ Search & Filter ‚îÄ‚îÄ
st.subheader("Buscar t√©rminos")
col_search, col_cat = st.columns([2, 1])
with col_search:
    search = st.text_input("Buscar por nombre o descripci√≥n", placeholder="Ej: conformal, PD, IFRS...")
with col_cat:
    categories = sorted({g["categoria"] for g in GLOSSARY})
    selected_cat = st.selectbox("Filtrar por categor√≠a", ["Todas"] + categories)

filtered = GLOSSARY
if search:
    search_lower = search.lower()
    filtered = [
        g for g in filtered
        if search_lower in g["termino"].lower()
        or search_lower in g["definicion"].lower()
        or search_lower in g["en_proyecto"].lower()
    ]
if selected_cat != "Todas":
    filtered = [g for g in filtered if g["categoria"] == selected_cat]

st.markdown(f"**{len(filtered)}** t√©rminos encontrados")

df_glossary = pd.DataFrame(filtered)
if not df_glossary.empty:
    df_display = df_glossary.rename(columns={
        "termino": "T√©rmino",
        "categoria": "Categor√≠a",
        "definicion": "Definici√≥n",
        "en_proyecto": "En este proyecto",
    })
    st.dataframe(df_display, use_container_width=True, hide_index=True, height=500)

# ‚îÄ‚îÄ Industry Usage ‚îÄ‚îÄ
st.subheader("T√©cnicas y su uso en la industria")
st.markdown(
    """
Las t√©cnicas empleadas en este proyecto no son experimentales: son herramientas utilizadas
activamente en bancos, fintechs y aseguradoras de primer nivel a nivel mundial.
"""
)

industry_data = [
    {"T√©cnica": "CatBoost / XGBoost / LightGBM", "Uso en la industria": "Credit scoring en >70% de instituciones financieras digitales. Dominantes en competencias Kaggle de datos tabulares. Adoptados por JPMorgan, Capital One, Nubank, Mercado Libre.", "En este proyecto": f"Modelo PD principal (CatBoost tuneado + {best_calibration})"},
    {"T√©cnica": "WOE / IV (Weight of Evidence)", "Uso en la industria": "Est√°ndar de facto en credit scoring bancario desde los a√±os 90. Requerido por algunos reguladores para scorecard interpretable.", "En este proyecto": "Feature engineering: grade_woe, purpose_woe, home_ownership_woe"},
    {"T√©cnica": "SHAP (Explicabilidad)", "Uso en la industria": "Est√°ndar de explicabilidad ML en banca (requerido por EBA, OCC). Usado para explicar decisiones individuales de cr√©dito.", "En este proyecto": "Top 20 features con SHAP, dependence plots"},
    {"T√©cnica": "Conformal Prediction", "Uso en la industria": "Adoptado en farmac√©utica (AstraZeneca), manufactura (Volvo), fintech (cuantificaci√≥n de incertidumbre en modelos de pricing y riesgo). Crecimiento exponencial desde 2020.", "En este proyecto": "MAPIE Mondrian: intervalos PD con cobertura garantizada por grade"},
    {"T√©cnica": "Inferencia Causal (DML/CATE)", "Uso en la industria": "Pricing din√°mico en Uber/Lyft, campa√±as de retenci√≥n en telecoms, an√°lisis de impacto de pol√≠ticas en banca central (BIS, Fed).", "En este proyecto": "Efecto tasa‚Üídefault, pol√≠ticas de intervenci√≥n por segmento"},
    {"T√©cnica": "Survival Analysis", "Uso en la industria": "Estimaci√≥n de lifetime PD para IFRS9 Stage 2 en todos los bancos bajo IFRS. Modelos de churn en telecoms y seguros.", "En este proyecto": f"Cox PH (C={survival_metrics.get('cox_concordance', 0):.4f}) y RSF (C={survival_metrics.get('rsf_concordance', 0):.4f}) para PD lifetime por grade"},
    {"T√©cnica": "Optimizaci√≥n Robusta (Pyomo)", "Uso en la industria": "Asignaci√≥n de capital en fondos de inversi√≥n, planificaci√≥n de supply chain (Amazon, Walmart), gesti√≥n de portafolio en asset management.", "En este proyecto": "Asignaci√≥n de pr√©stamos con uncertainty sets conformales + HiGHS solver"},
    {"T√©cnica": "IFRS9 / ECL Modeling", "Uso en la industria": "Obligatorio para todas las instituciones financieras bajo IFRS (>140 pa√≠ses). Cada banco tiene modelos internos de ECL por stage.", "En este proyecto": "4 escenarios, sensibilidad PD√óLGD, staging con conformal width"},
    {"T√©cnica": "dbt + DuckDB", "Uso en la industria": "dbt: est√°ndar de transformaci√≥n de datos en startups y empresas data-driven (Spotify, GitLab). DuckDB: an√°lisis local sin servidor (reemplaza SQLite para anal√≠tica).", "En este proyecto": "19 modelos dbt sobre DuckDB local con linaje verificable"},
]
st.dataframe(pd.DataFrame(industry_data), use_container_width=True, hide_index=True)

# ‚îÄ‚îÄ Key Formulas ‚îÄ‚îÄ
st.subheader("F√≥rmulas clave")

col_f1, col_f2 = st.columns(2)

with col_f1:
    st.markdown("**Expected Credit Loss (ECL)**")
    st.latex(r"ECL = PD \times LGD \times EAD \times DF")
    st.caption("Donde DF = factor de descuento. PD a 12 meses (Stage 1) o lifetime (Stage 2).")

    st.markdown("**Coeficiente Gini**")
    st.latex(r"Gini = 2 \times AUC - 1")
    st.caption("Escala: 0 (sin poder discriminante) a 1 (discriminaci√≥n perfecta).")

    st.markdown("**Brier Score**")
    st.latex(r"Brier = \frac{1}{N}\sum_{i=1}^{N}(p_i - y_i)^2")
    st.caption("Error cuadr√°tico medio de probabilidades. Menor es mejor.")

with col_f2:
    st.markdown("**Cobertura Conformal**")
    st.latex(r"Coverage = \frac{1}{N}\sum_{i=1}^{N}\mathbb{1}[y_i \in C(x_i)]")
    st.caption("Proporci√≥n de valores reales dentro del intervalo predicho. Meta: ‚â•90% y ‚â•95%.")

    st.markdown("**Price of Robustness**")
    st.latex(r"PoR = \frac{R_{nominal} - R_{robust}}{R_{nominal}} \times 100\%")
    st.caption("Porcentaje de retorno sacrificado por protecci√≥n contra incertidumbre.")

    st.markdown("**Information Value (IV)**")
    st.latex(r"IV = \sum_{i=1}^{B}(D_i\% - ND_i\%) \times \ln\left(\frac{D_i\%}{ND_i\%}\right)")
    st.caption("Poder predictivo global de una variable. >0.3 = muy fuerte.")

# ‚îÄ‚îÄ Practical Decision Guide ‚îÄ‚îÄ
st.subheader("Gu√≠a pr√°ctica: cu√°ndo elegir cada estrategia")
st.markdown(
    """
Esta secci√≥n traduce m√©tricas a decisiones reales. La pregunta no es solo "qu√© n√∫mero subi√≥ o baj√≥",
sino **qu√© pol√≠tica conviene seg√∫n objetivo de negocio**.
"""
)
st.caption(
    "Documento extendido en repositorio: "
    "`reports/guia_metricas_decision_negocio_vs_papers_2026-02-20.md`."
)

rob_summary = try_load_parquet("portfolio_robustness_summary")
rob_frontier = try_load_parquet("portfolio_robustness_frontier")

if rob_summary.empty or rob_frontier.empty:
    st.info("No se encontraron artefactos de robustez para construir la gu√≠a de perfiles.")
else:
    profile_cfg = pd.DataFrame(
        [
            {
                "Perfil": "Retorno",
                "risk_target": 0.12,
                "lambda_target": 0.0,
                "Cu√°ndo usarlo": "Objetivo comercial agresivo, tolerancia alta a volatilidad.",
                "Impacto negocio esperado": "Mayor upside de retorno, menor colch√≥n ante deterioro inesperado.",
            },
            {
                "Perfil": "Balanceado",
                "risk_target": 0.10,
                "lambda_target": 0.0,
                "Cu√°ndo usarlo": "Operaci√≥n est√°ndar con metas simult√°neas de crecimiento y control.",
                "Impacto negocio esperado": "Compromiso razonable entre rentabilidad y resiliencia.",
            },
            {
                "Perfil": "Prudente",
                "risk_target": 0.06,
                "lambda_target": 2.0,
                "Cu√°ndo usarlo": "Contexto de estr√©s, foco en preservaci√≥n de capital y estabilidad.",
                "Impacto negocio esperado": "Menor retorno y volumen financiado, mayor protecci√≥n en peor caso.",
            },
        ]
    )

    rows: list[dict[str, object]] = []
    robust_only = rob_frontier[rob_frontier["policy"] == "robust"].copy()
    for _, cfg in profile_cfg.iterrows():
        risk_target = float(cfg["risk_target"])
        lam_target = float(cfg["lambda_target"])

        robust_slice = robust_only.copy()
        robust_slice["_risk_dist"] = (robust_slice["risk_tolerance"] - risk_target).abs()
        robust_slice["_lam_dist"] = (robust_slice["uncertainty_aversion"] - lam_target).abs()
        robust_row = robust_slice.sort_values(["_risk_dist", "_lam_dist"]).iloc[0]

        summary_slice = rob_summary.copy()
        summary_slice["_risk_dist"] = (summary_slice["risk_tolerance"] - risk_target).abs()
        summary_row = summary_slice.sort_values("_risk_dist").iloc[0]

        rows.append(
            {
                "Perfil": cfg["Perfil"],
                "Par√°metros": (
                    f"risk_tolerance={robust_row['risk_tolerance']:.2f}, "
                    f"lambda={robust_row['uncertainty_aversion']:.1f}"
                ),
                "Retorno robusto": float(robust_row["expected_return_net_point"]),
                "Retorno no robusto": float(summary_row["baseline_nonrobust_return"]),
                "Price of Robustness (%)": float(robust_row["price_of_robustness_pct"]),
                "Worst-case PD": float(robust_row["worst_case_pd"]),
                "N financiados (robusto)": int(robust_row["n_funded"]),
                "Cu√°ndo usarlo": str(cfg["Cu√°ndo usarlo"]),
                "Impacto negocio esperado": str(cfg["Impacto negocio esperado"]),
            }
        )

    profiles_df = pd.DataFrame(rows)
    profiles_view = profiles_df.copy()
    profiles_view["Retorno robusto"] = profiles_view["Retorno robusto"].map(
        lambda v: format_number(float(v), prefix="$")
    )
    profiles_view["Retorno no robusto"] = profiles_view["Retorno no robusto"].map(
        lambda v: format_number(float(v), prefix="$")
    )
    profiles_view["Price of Robustness (%)"] = profiles_view["Price of Robustness (%)"].map(
        lambda v: f"{float(v):.2f}%"
    )
    profiles_view["Worst-case PD"] = profiles_view["Worst-case PD"].map(
        lambda v: format_pct(float(v), decimals=1)
    )
    st.dataframe(profiles_view, use_container_width=True, hide_index=True)

    st.markdown(
        """
**Regla r√°pida de decisi√≥n**

1. Si la prioridad es crecer retorno: usa **Retorno**.
2. Si la prioridad es operar estable todo el a√±o: usa **Balanceado**.
3. Si la prioridad es proteger capital en contexto adverso: usa **Prudente**.
"""
    )

st.subheader("Negocio vs papers: ¬øqu√© tan adoptado est√° cada enfoque?")
adoption_df = pd.DataFrame(
    [
        {
            "Pr√°ctica": "AUC / KS para discriminaci√≥n de score",
            "En negocio": "Muy adoptado",
            "En papers": "Muy adoptado",
            "Qu√© implica para el lector": "Es el est√°ndar para evaluar ranking de riesgo.",
        },
        {
            "Pr√°ctica": "Brier / ECE para calibraci√≥n",
            "En negocio": "Adoptado en equipos maduros de riesgo",
            "En papers": "Muy adoptado",
            "Qu√© implica para el lector": "Clave cuando PD se usa para pricing, l√≠mites e IFRS9.",
        },
        {
            "Pr√°ctica": "Conformal prediction para intervalos de PD",
            "En negocio": "Adopci√≥n emergente",
            "En papers": "Crecimiento fuerte",
            "Qu√© implica para el lector": "Aporta garant√≠a de cobertura y mejor gesti√≥n de incertidumbre.",
        },
        {
            "Pr√°ctica": "Optimizaci√≥n robusta con uncertainty sets",
            "En negocio": "Adopci√≥n selectiva (casos de alto impacto)",
            "En papers": "Bien establecida",
            "Qu√© implica para el lector": "Hace expl√≠cito el trade-off entre retorno y protecci√≥n.",
        },
        {
            "Pr√°ctica": "Price of Robustness como KPI formal",
            "En negocio": "Menos com√∫n como KPI expl√≠cito",
            "En papers": "Muy com√∫n",
            "Qu√© implica para el lector": "Sirve para explicar al negocio el costo del ‚Äúseguro‚Äù de robustez.",
        },
        {
            "Pr√°ctica": "IFRS9 Stage + escenarios ECL",
            "En negocio": "Obligatorio bajo IFRS",
            "En papers": "Muy estudiado",
            "Qu√© implica para el lector": "No es opcional; impacta provisi√≥n, capital y resultados.",
        },
    ]
)
st.dataframe(adoption_df, use_container_width=True, hide_index=True)

with st.expander("Guion de 1 minuto para explicarlo sin tecnicismos"):
    st.markdown(
        """
Nuestro modelo no solo ordena riesgo (AUC/KS), tambi√©n produce probabilidades confiables (Brier/ECE).
Luego le agregamos bandas de incertidumbre (conformal) para no decidir ‚Äúa ciegas‚Äù.
Con esas bandas, comparamos dos pol√≠ticas: una que maximiza retorno y otra que protege peor caso.
La diferencia entre ambas es el Price of Robustness: cu√°nto pagamos por estabilidad.
Finalmente, traducimos todo a provisiones IFRS9 para ver impacto contable real.
"""
    )

# ‚îÄ‚îÄ Reading Guide ‚îÄ‚îÄ
st.subheader("Gu√≠a de lectura del dashboard")
st.markdown(
    """
| Orden | Secci√≥n | P√°gina | Pregunta que responde |
|:-----:|---------|--------|-----------------------|
| 1 | Inicio | üè† Resumen Ejecutivo | ¬øQu√© problema resolvemos y con qu√© resultados? |
| 2 | Inicio | üìñ Glosario y Fundamentos (esta p√°gina) | ¬øQu√© significa cada t√©rmino y t√©cnica? |
| 3 | Recorrido E2E | üß≠ Visi√≥n End-to-End | ¬øCu√°l es la narrativa completa del proyecto? |
| 4 | Recorrido E2E | üóÇÔ∏è Arquitectura y Linaje de Datos | ¬øC√≥mo fluyen los datos a trav√©s del sistema? |
| 5 | Recorrido E2E | üß© Mapa Integrado de M√©todos | ¬øC√≥mo se conectan las t√©cnicas entre s√≠? |
| 6 | Recorrido E2E | üìö Atlas de Evidencia | ¬øD√≥nde est√° la evidencia de cada notebook? |
| 7 | Anal√≠tica | üîß Ingenier√≠a de Features | ¬øC√≥mo se transformaron las variables para el modelo? |
| 8 | Anal√≠tica | üìä Historia de Datos | ¬øQu√© contiene el dataset y qu√© patrones existen? |
| 9 | Anal√≠tica | üî¨ Laboratorio de Modelos | ¬øQu√© modelo se eligi√≥ y por qu√©? |
| 10 | Anal√≠tica | üìê Cuantificaci√≥n de Incertidumbre | ¬øC√≥mo cuantificamos la incertidumbre de las predicciones? |
| 11 | Anal√≠tica | üìà Panorama Temporal | ¬øC√≥mo evolucionan los defaults en el tiempo? |
| 12 | Anal√≠tica | ‚è≥ An√°lisis de Supervivencia | ¬øCu√°ndo ocurren los defaults? |
| 13 | Anal√≠tica | üß¨ Inteligencia Causal | ¬øQu√© intervenciones pueden reducir el riesgo? |
| 14 | Decisiones | üíº Optimizador de Portafolio | ¬øC√≥mo asignar capital bajo incertidumbre? |
| 15 | Decisiones | üè¶ Provisiones IFRS9 | ¬øCu√°nto provisionar bajo diferentes escenarios? |
| 16 | Gobernanza | üõ°Ô∏è Gobernanza del Modelo | ¬øEs el modelo confiable y justo? |
| 17 | Exploraci√≥n | üí¨ Chat con Datos | Exploraci√≥n libre por SQL |
"""
)

next_page_teaser(
    "Historia de Datos",
    "Explora el dataset: distribuciones, patrones de riesgo y din√°mica temporal de 1.35M pr√©stamos.",
    "pages/data_story.py",
)
