"""Stack tecnologico: librerias, versiones, practicas de ingenieria."""

from __future__ import annotations

import pandas as pd
import streamlit as st
from streamlit_mermaid import st_mermaid

from streamlit_app.components.narrative import next_page_teaser
from streamlit_app.utils import load_runtime_status


def _lib_df(rows: list[list[str]]) -> pd.DataFrame:
    """Build a library comparison DataFrame."""
    return pd.DataFrame(
        rows, columns=["Librer√≠a", "Versi√≥n", "Rol", "Alternativas", "Raz√≥n de elecci√≥n"]
    )


st.title("üõ†Ô∏è Stack Tecnol√≥gico")
st.caption(
    "Librer√≠as, versiones, decisiones de dise√±o y pr√°cticas de ingenier√≠a "
    "del pipeline de riesgo de cr√©dito."
)

st.markdown(
    """
Documentar el stack tecnol√≥gico no es un ejercicio cosm√©tico: permite reproducibilidad,
facilita la colaboraci√≥n, y demuestra que cada componente fue seleccionado con criterio
t√©cnico. Esta p√°gina detalla **por qu√©** cada librer√≠a y no solo **cu√°l**.
"""
)

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# 1. Library Ecosystem
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
st.subheader("1) Ecosistema de librer√≠as")

tabs = st.tabs(
    [
        "ML Core",
        "Conformal",
        "Series de Tiempo",
        "Supervivencia",
        "Causal",
        "Optimizaci√≥n",
        "Datos",
        "MLOps",
        "API & Dashboard",
        "Plataforma",
        "Dev",
    ]
)

with tabs[0]:
    st.dataframe(
        _lib_df(
            [
                [
                    "catboost",
                    ">=1.2",
                    "PD model principal",
                    "XGBoost, LightGBM",
                    "Manejo nativo de categor√≠as y NaN ‚Äî cero preprocesamiento",
                ],
                [
                    "scikit-learn",
                    ">=1.6",
                    "Framework base ML",
                    "‚Äî",
                    "Est√°ndar de la industria; utilidades de validaci√≥n y calibraci√≥n probabil√≠stica",
                ],
                [
                    "lightgbm",
                    ">=4.5",
                    "Time series v√≠a mlforecast",
                    "XGBoost",
                    "M√°s r√°pido en entrenamiento, integraci√≥n Nixtla nativa",
                ],
                [
                    "optuna",
                    ">=4.1",
                    "Hyperparameter tuning",
                    "Hyperopt, Ray Tune",
                    "API pyth√≥nica, pruning inteligente, visualizaci√≥n integrada",
                ],
                [
                    "shap",
                    ">=0.47",
                    "Explicabilidad del modelo",
                    "LIME, ELI5",
                    "Teor√≠a s√≥lida (Shapley values), TreeSHAP para CatBoost",
                ],
                [
                    "optbinning",
                    ">=0.19",
                    "WOE binning",
                    "Binning manual",
                    "Binning √≥ptimo con regularizaci√≥n; calcula IV autom√°ticamente",
                ],
            ]
        ),
        use_container_width=True,
        hide_index=True,
    )

with tabs[1]:
    st.dataframe(
        _lib_df(
            [
                [
                    "mapie",
                    ">=1.3.0,<2",
                    "Conformal prediction",
                    "crepes, nonconformist",
                    "API moderna: SplitConformalRegressor + Mondrian nativo",
                ],
                [
                    "crepes",
                    ">=0.7",
                    "CP complementario",
                    "mapie",
                    "Implementaci√≥n alternativa para validaci√≥n cruzada de resultados",
                ],
            ]
        ),
        use_container_width=True,
        hide_index=True,
    )
    st.info(
        "**Decisi√≥n cr√≠tica**: MAPIE >=1.3.0 (no >=0.9). La API cambi√≥ "
        "completamente: `SplitConformalRegressor` reemplaza `MapieRegressor`, "
        "`conformalize()` reemplaza `fit()` con `cv='prefit'`."
    )

with tabs[2]:
    st.dataframe(
        _lib_df(
            [
                [
                    "statsforecast",
                    ">=2.0",
                    "Modelos estad√≠sticos (ARIMA, ETS)",
                    "statsmodels, Prophet",
                    "10-100x m√°s r√°pido que statsmodels; API consistente",
                ],
                [
                    "mlforecast",
                    ">=0.13",
                    "ML para time series (LightGBM)",
                    "Prophet, sktime",
                    "Integraci√≥n nativa con LightGBM; feature engineering temporal",
                ],
                [
                    "hierarchicalforecast",
                    ">=1.0",
                    "Reconciliaci√≥n jer√°rquica",
                    "‚Äî",
                    "√önico ecosistema open-source para forecasting jer√°rquico",
                ],
            ]
        ),
        use_container_width=True,
        hide_index=True,
    )
    st.caption("Ecosistema Nixtla: las tres librer√≠as comparten API y son interoperables.")

with tabs[3]:
    st.dataframe(
        _lib_df(
            [
                [
                    "lifelines",
                    ">=0.30",
                    "Cox PH, Kaplan-Meier",
                    "R survival",
                    "API pyth√≥nica, estimaci√≥n semiparam√©trica, Schoenfeld tests",
                ],
                [
                    "scikit-survival",
                    ">=0.24",
                    "Random Survival Forests",
                    "pycox (deep learning)",
                    "RSF interpretable; compatible con sklearn API",
                ],
            ]
        ),
        use_container_width=True,
        hide_index=True,
    )

with tabs[4]:
    st.dataframe(
        _lib_df(
            [
                [
                    "econml",
                    ">=0.16",
                    "DML, Causal Forest, CATE",
                    "CausalML (Uber)",
                    "Microsoft Research; DML + orthogonalization rigurosa",
                ],
                [
                    "dowhy",
                    ">=0.12",
                    "Grafos causales, refutaci√≥n",
                    "‚Äî",
                    "√önico framework Python con refutation tests formales",
                ],
            ]
        ),
        use_container_width=True,
        hide_index=True,
    )

with tabs[5]:
    st.dataframe(
        _lib_df(
            [
                [
                    "pyomo",
                    ">=6.8",
                    "Modelado algebraico (LP/MILP)",
                    "PuLP, CVXPY",
                    "M√°s expresivo que PuLP; soporta NLP y robust optimization",
                ],
                [
                    "highspy",
                    ">=1.10",
                    "Solver HiGHS (LP/MIP)",
                    "Gurobi (comercial), CBC",
                    "Open-source, rendimiento comparable a Gurobi en LPs medianos",
                ],
                [
                    "cvxpy",
                    ">=1.6",
                    "Optimizaci√≥n convexa",
                    "scipy.optimize",
                    "Disciplined convex programming; √∫til para formulaciones alternativas",
                ],
            ]
        ),
        use_container_width=True,
        hide_index=True,
    )
    st.info(
        "**HiGHS vs Gurobi**: HiGHS es open-source y resuelve nuestro portfolio LP "
        "en <1s. Gurobi ser√≠a necesario solo para MILPs de >100K variables."
    )

with tabs[6]:
    st.dataframe(
        _lib_df(
            [
                [
                    "pandas",
                    ">=2.2",
                    "Data wrangling principal",
                    "polars",
                    "Ecosistema maduro; compatibilidad universal",
                ],
                [
                    "polars",
                    ">=1.20",
                    "Procesamiento high-performance",
                    "pandas",
                    "10x m√°s r√°pido en operaciones pesadas; lazy evaluation",
                ],
                [
                    "duckdb",
                    ">=1.1",
                    "Queries anal√≠ticas in-process",
                    "SQLite",
                    "OLAP nativo; lee parquet directamente; backend para dbt",
                ],
                [
                    "pyarrow",
                    ">=18.0",
                    "I/O parquet, interop columnar",
                    "fastparquet",
                    "Est√°ndar de facto; interop con pandas, polars, duckdb",
                ],
            ]
        ),
        use_container_width=True,
        hide_index=True,
    )

with tabs[7]:
    st.dataframe(
        _lib_df(
            [
                [
                    "dvc",
                    ">=3.56",
                    "Versionado de datos",
                    "git-lfs",
                    "Pipeline DAG + remote storage; git-lfs no soporta pipelines",
                ],
                [
                    "mlflow",
                    ">=2.20",
                    "Experiment tracking",
                    "W&B (comercial)",
                    "Open-source, UI integrada, model registry",
                ],
                [
                    "pandera",
                    ">=0.22",
                    "Validaci√≥n de schemas",
                    "great_expectations",
                    "Ligero, decoradores, integraci√≥n pandas nativa",
                ],
                [
                    "dagshub",
                    ">=0.4",
                    "Hub MLOps (Git + DVC + MLflow)",
                    "‚Äî",
                    "Unifica Git mirror, DVC remote y MLflow tracking en una plataforma",
                ],
            ]
        ),
        use_container_width=True,
        hide_index=True,
    )

with tabs[8]:
    st.dataframe(
        _lib_df(
            [
                [
                    "fastapi",
                    ">=0.115",
                    "API REST",
                    "Flask, Django",
                    "Async nativo, auto-docs OpenAPI, Pydantic validation",
                ],
                [
                    "streamlit",
                    ">=1.42",
                    "Dashboard interactivo",
                    "Dash, Gradio",
                    "Prototipado r√°pido; st.navigation() para multi-page apps",
                ],
                [
                    "mcp[cli]",
                    ">=1.2",
                    "Model Context Protocol",
                    "‚Äî",
                    "Protocolo est√°ndar para LLM tool use; FastMCP server",
                ],
            ]
        ),
        use_container_width=True,
        hide_index=True,
    )

with tabs[9]:
    st.dataframe(
        _lib_df(
            [
                [
                    "dbt-duckdb",
                    ">=1.9",
                    "Transformaci√≥n SQL gobernada",
                    "SQL scripts manuales",
                    "Lineage, tests, docs autom√°ticos; read_parquet() en staging",
                ],
                [
                    "feast",
                    ">=0.55",
                    "Feature store",
                    "Tecton (comercial)",
                    "Feature views declarativos; offline/online serving",
                ],
            ]
        ),
        use_container_width=True,
        hide_index=True,
    )

with tabs[10]:
    st.dataframe(
        _lib_df(
            [
                [
                    "pytest",
                    ">=8.3",
                    "Test framework",
                    "unittest",
                    "Fixtures, markers, plugins; est√°ndar de facto en Python moderno",
                ],
                [
                    "ruff",
                    ">=0.9",
                    "Linter + formatter",
                    "flake8+isort+black",
                    "Una herramienta reemplaza tres; 10-100x m√°s r√°pido (Rust)",
                ],
                [
                    "pre-commit",
                    ">=4.0",
                    "Git hooks autom√°ticos",
                    "husky (JS)",
                    "Est√°ndar Python; ejecuta Ruff + nbstripout antes de commit",
                ],
                [
                    "nbstripout",
                    ">=0.8",
                    "Limpiar outputs de notebooks",
                    "‚Äî",
                    "Previene merge conflicts en notebooks",
                ],
                [
                    "uv",
                    "latest",
                    "Package manager",
                    "pip, poetry, conda",
                    "10-100x m√°s r√°pido que pip; lock file determin√≠stico",
                ],
            ]
        ),
        use_container_width=True,
        hide_index=True,
    )

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# 2. Engineering Practices
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
st.subheader("2) Pr√°cticas de ingenier√≠a")

runtime_status = load_runtime_status()
TEST_SUITE_TOTAL = int(runtime_status.get("test_suite_total", 0) or 0)
PAGES_TOTAL = int(runtime_status.get("streamlit_pages_total", 0) or 0)
PAGES_LABEL = str(PAGES_TOTAL) if PAGES_TOTAL > 0 else "N/D"
_description_map = {
    "test_api/test_router_normalization": "Normalizaci√≥n de par√°metros FastAPI",
    "test_config_consistency": "Drift entre config YAML, contrato PD y DAG DVC",
    "test_data/test_make_dataset": "Limpieza inicial y control de leakage",
    "test_data/test_prepare_dataset": "Splits OOT y calibraci√≥n temporal",
    "test_data/test_build_datasets": "loan_master/time_series/ead + parsing robusto",
    "test_features/test_feature_engineering": "Feature engineering",
    "test_features/test_schemas": "Validaci√≥n de esquemas Pandera",
    "test_models/test_pd_model": "CatBoost, baseline LR, calibraci√≥n",
    "test_models/test_conformal": "Cobertura, grupos, Mondrian y edge cases",
    "test_evaluation/test_ifrs9": "Staging ECL, rangos conformal y edge cases",
    "test_evaluation/test_metrics": "M√©tricas clasificaci√≥n y regresi√≥n",
    "test_optimization/test_portfolio": "Pyomo, solver, constraints y escenarios",
    "test_scripts/test_end_to_end_pipeline": "Orquestaci√≥n fail-fast/tolerante",
    "test_scripts/test_export_streamlit_artifacts": "Contrato de export para Streamlit",
    "test_scripts/test_mlflow_suite": "Suite de logging MLflow/DagsHub",
    "test_utils/test_mlflow_utils": "init_dagshub + logging de experimentos",
    "test_streamlit/test_page_imports": "Smoke AST/import de p√°ginas Streamlit",
    "test_integration": "Pipeline completo sobre datos sint√©ticos",
    "test_docs/test_narrative_consistency": "Guard de claims narrativos",
}
TEST_BREAKDOWN = [
    [
        row["module"],
        int(row["tests"]),
        _description_map.get(str(row["module"]), "Cobertura autom√°tica de pytest"),
    ]
    for row in runtime_status.get("test_breakdown", [])
]
if not TEST_BREAKDOWN:
    TEST_BREAKDOWN = [["N/D", 0, "No se pudo recolectar inventario de tests"]]

with st.expander(f"Testing: {TEST_SUITE_TOTAL} tests con pytest"):
    test_data = pd.DataFrame(
        TEST_BREAKDOWN,
        columns=["M√≥dulo", "Tests", "Qu√© valida"],
    )
    st.dataframe(test_data, use_container_width=True, hide_index=True)
    st.markdown(
        """
**Patrones de testing**:
- Fixtures con datos sint√©ticos (no dependen de archivos reales)
- Markers: `@pytest.mark.slow` para tests pesados, `@pytest.mark.integration`
- Config consistency tests: previenen drift entre YAML, c√≥digo y tesis
- `pytest -x`: falla r√°pido en el primer error
"""
    )

with st.expander("Linting: Ruff con reglas E, F, W, I, UP, B, SIM"):
    st.markdown(
        """
**Ruff** reemplaza tres herramientas (flake8 + isort + black) con una sola, escrita en
Rust y 10-100x m√°s r√°pida.

**Reglas activas**:
| C√≥digo | Categor√≠a | Qu√© detecta |
|--------|-----------|-------------|
| E | pycodestyle | Espacios, indentaci√≥n, l√≠neas largas |
| F | Pyflakes | Imports no usados, variables undefined |
| W | Warnings | Deprecations, estilo |
| I | isort | Orden de imports |
| UP | pyupgrade | Idiomas modernos de Python 3.11+ |
| B | flake8-bugbear | Pitfalls comunes |
| SIM | flake8-simplify | Simplificaciones de c√≥digo |

**Config**: `line-length = 100`, `target-version = "py311"`, `known-first-party = ["src"]`
"""
    )

with st.expander("Pre-commit hooks"):
    st.markdown(
        """
Dos hooks ejecutan autom√°ticamente antes de cada `git commit`:

1. **Ruff** (`ruff --fix` + `ruff format`): auto-corrige imports, formato y estilo.
2. **nbstripout**: elimina outputs y metadata de notebooks, previniendo merge conflicts
   y evitando que datos sensibles se filtren al repositorio.

Config en `.pre-commit-config.yaml` con `ruff-pre-commit v0.9.0` y `nbstripout 0.8.1`.
"""
    )

with st.expander("CI/CD: GitHub Actions (3 jobs)"):
    st.markdown(
        f"""
**Workflow**: `.github/workflows/ci.yml` ‚Äî triggers en push a main/master y PRs.

| Job | Qu√© hace | Comando |
|-----|----------|---------|
| **Lint** | Verifica formato y estilo | `ruff check` + `ruff format --check` |
| **Test** | Ejecuta test suite completa | `pytest -x -m "not slow"` |
| **Streamlit Smoke** | Verifica que las {PAGES_LABEL} p√°ginas importan sin error | `importlib.util.find_spec()` |

Setup: `actions/checkout@v4` + `astral-sh/setup-uv@v5` + `uv sync --extra dev`.
"""
    )

with st.expander("Containerizaci√≥n: Docker Compose (2 servicios)"):
    st.markdown(
        f"""
| Servicio | Puerto | Base | Prop√≥sito |
|----------|--------|------|-----------|
| **api** | 8000 | python:3.12-slim | FastAPI con CatBoost inference |
| **streamlit** | 8501 | python:3.12-slim | Dashboard de {PAGES_LABEL} p√°ginas |

**Pr√°cticas clave**:
- Data y modelos montados como **vol√∫menes read-only** (inmutabilidad)
- `uv sync --frozen` para instalaci√≥n determin√≠stica
- Healthcheck HTTP en el servicio API
- Multi-stage build con `ghcr.io/astral-sh/uv:latest`
"""
    )

with st.expander("Gobernanza de datos: dbt + Feast"):
    st.markdown(
        """
**dbt-duckdb**: 19 modelos SQL (9 tables + 10 views), 16 tests de integridad.
- Staging: `read_parquet()` directo sobre archivos parquet
- Marts: tablas anal√≠ticas listas para consumo (risk assessment, ECL, conformal, causal)
- Tests: not_null, unique, accepted_values en columnas cr√≠ticas

**Feast**: 3 feature views + 2 feature services.
- `loan_origination`: features de originaci√≥n (monto, ingreso, DTI, tasa)
- `loan_credit_history`: historial crediticio (delinquencias, cuentas abiertas)
- `loan_demographics`: demograf√≠a (vivienda, prop√≥sito, empleo)
- Registry SQLite local; offline store en parquets
"""
    )

with st.expander("Configuraci√≥n: YAML + Pandera"):
    st.markdown(
        """
**4 archivos YAML** en `configs/`:
- `pd_model.yaml`: hiperpar√°metros CatBoost, paths, features, calibraci√≥n, conformal
- `conformal_policy.yaml`: umbrales de cobertura, alertas, artefactos
- `optimization.yaml`: tipo de solver, presupuesto, concentraci√≥n, robustez
- `modeva_governance.yaml`: fairness, drift, robustness diagnostics

**Pandera schemas** en `src/features/schemas.py`: validaci√≥n en boundaries del pipeline.
Cada DataFrame pasa por un schema check antes de persistirse o consumirse downstream.

**Tests de consistencia** (`tests/test_config_consistency.py`): 7 tests que verifican
que config YAML, c√≥digo fuente y tesis est√©n alineados ‚Äî previenen drift silencioso.
"""
    )

with st.expander("Logging: loguru (no print)"):
    st.markdown(
        """
`from loguru import logger` reemplaza `print()` en todo el codebase.

**Ventajas**: timestamps autom√°ticos, niveles (INFO/WARNING/ERROR),
formato estructurado, rotaci√≥n de archivos, stack traces enriquecidos.

Convenci√≥n: `logger.info(f"...")` para progreso, `logger.warning(...)` para
condiciones recuperables, `logger.error(...)` para fallos.
"""
    )

with st.expander("Gesti√≥n de paquetes: uv"):
    st.markdown(
        """
**uv** (Astral) reemplaza pip, poetry y conda con una herramienta 10-100x m√°s r√°pida
escrita en Rust.

- `uv sync --extra dev`: instala dependencias de desarrollo (pytest, ruff)
- `uv sync --extra platform`: instala dbt + Feast (incompatible con dev)
- `uv.lock`: lock file frozen para reproducibilidad exacta
- `.venv/`: virtualenv Linux nativo en WSL2

**Nota**: los extras `dev` y `platform` no pueden coexistir debido a conflictos
de versiones (feast requiere numpy>=2, mpi-sppy requiere numpy<2).
"""
    )

with st.expander("MLOps: DVC Pipeline + MLflow Tracking"):
    st.markdown(
        """
**DVC** (Data Version Control) define el pipeline completo como un DAG declarativo
en `dvc.yaml` con **17 stages**, desde la limpieza de datos hasta la capa de export/storytelling.

```text
make_dataset ‚Üí prepare_dataset ‚Üí build_datasets
  ‚Üí train_pd_model ‚Üí generate_conformal
  ‚Üí forecast_default_rates
  ‚Üí estimate_causal_effects ‚Üí simulate_causal_policy ‚Üí validate_causal_policy
  ‚Üí run_ifrs9_sensitivity
  ‚Üí optimize_portfolio ‚Üí optimize_portfolio_tradeoff
  ‚Üí run_survival_analysis
  ‚Üí backtest_conformal_coverage ‚Üí validate_conformal_policy
  ‚Üí export_streamlit_artifacts ‚Üí export_storytelling_snapshot
```

- `dvc.yaml`: define dependencias, comandos y salidas de cada stage
- `dvc.lock`: fija hashes de datos y artefactos para reproducibilidad exacta
- `dvc repro`: re-ejecuta solo los stages con dependencias modificadas
- Remote: **DagsHub** (almacenamiento centralizado de artefactos grandes)

**MLflow** registra 8 experimentos desde artefactos existentes:
1. `end_to_end` ‚Äî m√©tricas globales del pipeline
2. `pd_model` ‚Äî AUC, Gini, KS, Brier, ECE del modelo CatBoost calibrado
3. `conformal` ‚Äî cobertura Mondrian 90%/95%, ancho de intervalos
4. `causal_policy` ‚Äî regla seleccionada, net value, bootstrap p05
5. `ifrs9` ‚Äî ECL baseline/severe, uplift por escenario
6. `optimization` ‚Äî portafolio robusto vs nominal, costo de robustez
7. `survival` ‚Äî Cox C-index, RSF C-index, concordancia
8. `time_series` ‚Äî MASE, RMSSE por modelo de forecast

**DagsHub** unifica las tres capas:
- **Git mirror**: sincronizaci√≥n autom√°tica con GitHub
- **DVC remote**: almacenamiento de parquets y modelos serializados
- **MLflow UI**: visualizaci√≥n de experimentos, comparaci√≥n de runs, registro de modelos
"""
    )

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# 3. System Architecture
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
st.subheader("3) Arquitectura del sistema")

st_mermaid(
    """
    graph LR
        subgraph Data["Capa de Datos"]
            A[CSV/Parquet] --> B[DuckDB]
            B --> C[dbt Models]
            C --> D[Feast Features]
        end
        subgraph ML["Capa ML"]
            D --> E[CatBoost PD]
            E --> F[Calibraci√≥n probabil√≠stica]
        end
        subgraph CP["Capa Conformal"]
            F --> G[MAPIE Mondrian]
            G --> H["[PD_low, PD_high]"]
        end
        subgraph OR["Capa Optimizaci√≥n"]
            H --> I[Box Uncertainty Sets]
            I --> J[Pyomo + HiGHS]
        end
        subgraph Serving["Capa Serving"]
            J --> K[FastAPI]
            J --> L[Streamlit]
            J --> M[MCP Server]
        end

        style Data fill:#1a1a2e,stroke:#00D4AA,color:#e0e0e0
        style ML fill:#16213e,stroke:#00D4AA,color:#e0e0e0
        style CP fill:#16213e,stroke:#FFD93D,color:#e0e0e0
        style OR fill:#0f3460,stroke:#FF6B6B,color:#e0e0e0
        style Serving fill:#1a1a2e,stroke:#00D4AA,color:#e0e0e0
    """,
    height=300,
)

st.caption(
    "Cinco capas independientes conectadas por artefactos (parquets, modelos serializados, JSON contracts). "
    "Cada capa se puede reemplazar sin afectar las dem√°s."
)

next_page_teaser(
    "Chat con Datos",
    "Explora libremente marts y staging con SQL o consultas en lenguaje natural.",
    "pages/chat_with_data.py",
)
